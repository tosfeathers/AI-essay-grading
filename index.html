<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI essay scoring</title>

    <!-- Google fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,700;1,400;1,700&display=swap"
        rel="stylesheet">
    <style>
        * {
  box-sizing: border-box;
}
        
        body {
            margin: 0;
            font-size: 20px;
            font-family: Georgia, 'Times New Roman', Times, serif;
            text-rendering: optimizeLegibility;
        }

        .content {
            max-width: 640px;
            margin: auto;
        }

        .header {
            padding: 3em 0;
        }

        .chartheader {
            padding-top: 0em;
            padding-bottom: 1em;
        }

        a {
            color: #f05349;
        }

        .footer {
            background: #f4f4f4;
            text-align: center;
            font-size: 0.8em;
            margin-top: 4em;
            padding: 4em 0;
        }

        figure {
            margin: 0;
            padding-bottom: 1.2em;
        }

        figcaption {
            font-size: 0.8em;
            text-align: center;
            margin-top: 0.5em;
            color: #666;
        }

        h1 {
            font-family: 'Lora', Georgia, serif;
            font-weight: bold;
            font-size: 3em;
            line-height: 1.1;
        }

        .subhead {
            font-family: 'Lora', Georgia, serif;
        }

        iframe,
        img,
        video {
            max-width: 100%;
        }

        p {
            line-height: 1.6;
            margin: 0;
            padding-bottom: 1.2em;
        }

        ul {
            margin: 0;
            padding-bottom: 1em;
            line-height: 1.6;
        }

        iframe {
            padding-bottom: 1.2em;
        }

        code {
            font-family: 'Courier New', monospace;
            background: #fffbaf;
            font-size: 0.85em;
        }

        .full-width figure {
            text-align: center;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
        }
        /* Three image containers */
        .columntwo {
             float: left;
             width: 50%;
             padding: 5px;
        }
        .columnthree {
             float: left;
             width: 33.33%;
             padding: 5px;
        }


        /* Clear floats after image containers */
        .row::after {
        content: "";
        clear: both;
        display: table;
        }

        /* margin on mobile */
        @media (max-width: 640px) {
            body {
                font-size: 18px;
            }

            .content {
                padding-left: 0.5em;
                padding-right: 0.5em;
            }
        }
    </style>
</head>

<body>
    <div class="content">
        <div class="header">
            <h1>Popular AI essay graders often disagree with humans, and each other</h1>
            <p class="subhead">I ran 100 essays scored by expert human graders through Gammerly and Writable. These are the results.</p>
        </div>
        <div class="byline">
            <p>By <a href="https://toddfeathers.com">Todd Feathers</a></p>
        </div>  
            <p>The excerpt below is from an essay written by a 10th grade student responding to a prompt that asked whether emotion detection technology would be valuable in schools.</p>
            <p>The student's essay—and more than 2,000 others like it—were scored on a scale of 1 to 6 by two expert human graders. Any disagreements between the two scores were adjudicated by a third expert human grader.</p>
            <p>The human graders assigned this essay a score of 2 out of 6.</p>
    </div>           
    <div class="full-width">
        <figure>
            <img
                src="blue-essay.png">
        </figure>
    </div>
    <div class="content">   
            <p>The second excerpt below is from another essay in the same corpus of writing, called <a href="https://github.com/scrosseye/persuade_corpus_2.0">PERSUADE 2.0</a>, which was created to help AI developers train models that can grade persuasive writing.</p>
            <p>The humans graders also assigned this essay a score of 2 out of 6.</p>
    </div> 
    <div class="full-width">
        <figure>
            <img
                src="green-essay.png">
        </figure>
    </div>    
    <div class="content">   
        <p>While a panel of three experts arrived at the same score for these pieces of writing, two of the most popular AI writing tools marketed to teachers reached very different conclusions. The <span style="color: blue">blue</span> essay received a fairly low score of 25 out of 100 from Grammarly, but a relatively high score of 12.06 out of 18 from Writable. Meanwhile, the <span style="color: green">green</span> essay received a score of 73 from Grammarly but only 6.96 from Writable</p>          
        <p>I extracted 100 essays from the PERSUADE 2.0 corpus and ran them through the free versions of Grammarly and Writibale. The results show widespread disagreement about the quality of students' writings.</p>
    <div class="chartheader">
        <h4 align="center"> How Grammarly and Writable scores differed</h4>
    </div>  
        <figure>
            <img class="center"
                src="writable_grammarly_chart.png">
        </figure>     
        <p>The AI scores broadly corresponeded to human scores, with essays that received a score of 5 from humans receiving better Grammarly and Writable scores, on average, than essays that received a 4 from humans. The exception is that essays that received a 3 from the human graders had worse average Grammarly scores than essays that received a 2.</p>
        <p>But within any given human score bucket, both AI mdoels returned a wide range of different scores.</p>
    <div class="chartheader">
        <h4 align="center"> How AI scores and human scores differed</h4>
    </div>  
    <div class="row">
        <div class="columntwo">
          <img src="grammarly_human.png" style="width:100%">
        </div>
        <div class="columntwo">
          <img src="writable_human.png" style="width:100%">
        </div>
    </div> 
    <div class="content">   
        <p>There were some differences in the AI grading tools. Writable allowed me to input the essay prompt as part of its scoring, whereas Grammarly didn't. But in both cases, I was able to specify that the student was in high school and that they were writing a persuasive essay. Before displaying a score for the first essay it graded, Writable also delivered a pop up warning me that I shouldn't completely trust the algorithm and simply give my students their AI-generated grades.</p>
        <p>I randomly sampled the 100 essays from the PERSUADE 2.0 so that my test group had an equal number (20) of essays written by students who identified as White, Black, Hispanic, Asian or Pacific Islander, or two-or-more races.</p>
    </div>  
    </div>  
    <div class="chartheader">
        <h4 align="center"> Average scores for students of different races</h4>
    </div>  
        <figure>
            <img class="center"
                src="race_human.png">
        </figure>
        <figure>
            <img class="center"
                src="race_writable.png">
        </figure>
        <figure>
            <img class="center"
                src="race_grammarly.png">
        </figure>
    </div>
    <div class="content">   
        <p>Under all three grading systems, students who identified as White or two-or-more races had the highest average scores, while Black and Hispanic students had the lowest average scores.</p>
        <p>The PERSUADE 2.0 corpus also included English Language Learner (ELL) status for the essay writers. I found that the average human-graded score given to ELL students was 29 percent lower than the average score given to non-ELL students. The Grammarly results were similar, with the average score assigned to ELL students 28 percent lower than non-ELL students. There was less disparity in the Writable results: The tool's average score for ELL students was 14 percent lower than non-ELL students.</p>
        <p>However, it should be noted that my sample of 100 essays contained 74 written by students who were non-ELL and only 26 by ELL students.</p>
        <p>AI graders like Grammarly and writable are marketed to schools as time-savers for overworked teachers that can also provide immediate guidance to students who might otherwise have to wait weeks to receive feedback on their writing. While both those selling points may be true, the results of this experiment suggest that the feedback these tools provide isn't consistent. Academic experts who have <a href="https://hechingerreport.org/proof-points-ai-essay-grading/">researched</a> the quality of AI essay grading say these tools should only be used in low-stakes settings.</p>
    </div>
    <div class="footer">
        <div class="content">
            <p>You can find the github for this project <a href="https://github.com/tosfeathers/AI-essay-grading">here</a>.</p>
        </div>
    </div>
</body>

</html>